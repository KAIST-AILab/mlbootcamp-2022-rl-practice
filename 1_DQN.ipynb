{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tzs930/mlbootcamp-2022-rl-practice/blob/main/1_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "bcGe4g66VZTy"
      },
      "source": [
        "# Reinforcement Learning Practice 1 : DQN\n",
        "\n",
        "In this assignment, we will implement Q-learning algorithm based on Neural networks.\n",
        "\n",
        "We begin with basic package & environment setup as below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNeBe8cVXhah"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/MicrosoftLearning/Reinforcement-Learning-Explained.git && cd Reinforcement-Learning-Explained && mv * ../\n",
        "!rm -rf Reinforcement-Learning-Explained && rm -rf Module* && rm -rf sample_data\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8OkadhFVZT1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "if \"../\" not in sys.path:\n",
        "    sys.path.append(\"../\") \n",
        "\n",
        "from lib.envs.simple_rooms import SimpleRoomsEnv\n",
        "from lib.simulation import Experiment\n",
        "\n",
        "try:\n",
        "    import chainer\n",
        "except ImportError as e:\n",
        "    !pip install chainer\n",
        "    import chainer\n",
        "    \n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import initializers, optimizers, Chain, Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A67yzmQ1TBUp"
      },
      "source": [
        "### Implement DQN\n",
        "\n",
        "- Define DQN Agent\n",
        "- Recall that the loss function for Q-value is $\\left(r+\\gamma \\max_{a'}Q(s',a') - Q(s,a)\\right)^2$.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpzff9I9VZT3"
      },
      "outputs": [],
      "source": [
        "class Agent(object):  \n",
        "        \n",
        "    def __init__(self, actions):\n",
        "        self.actions = actions\n",
        "        self.num_actions = len(actions)\n",
        "\n",
        "    def act(self, state):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9adG1gFVZT3"
      },
      "outputs": [],
      "source": [
        "class DQLearningAgent(Agent):\n",
        "    \"\"\"Q-Learning agent with function approximation.\"\"\"\n",
        "\n",
        "    def __init__(self, actions, obs_size, **kwargs):\n",
        "        super(DQLearningAgent, self).__init__(actions)\n",
        "\n",
        "        self.obs_size = obs_size\n",
        "        \n",
        "        self.step_counter = 0\n",
        "        self.epsilon = kwargs.get('epsilon', .01)       \n",
        "        # if epsilon set to 1, it will be decayed over time\n",
        "        if self.epsilon == 1:\n",
        "            self.epsilon_decay = True\n",
        "        else:\n",
        "            self.epsilon_decay = False\n",
        "            \n",
        "        self.gamma = kwargs.get('gamma', .99)\n",
        "        \n",
        "        self.minibatch_size = kwargs.get('minibatch_size', 32)\n",
        "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
        "        self.tau = kwargs.get('tau', .001)\n",
        "        self.model_network = QNetwork(self.obs_size, self.num_actions, kwargs.get('nhidden', 512))\n",
        "        self.target_network = QNetwork(self.obs_size, self.num_actions, kwargs.get('nhidden', 512))\n",
        "        self.target_network.copyparams(self.model_network)\n",
        "        \n",
        "        self.optimizer = self.init_optimizer(self.model_network, kwargs.get('learning_rate', .5))\n",
        "\n",
        "        self.memory = ReplayMemory(self.obs_size, kwargs.get('mem_size', 10000))\n",
        "        \n",
        "        self.current_loss = .0\n",
        "    \n",
        "    def act(self, state):\n",
        "        \n",
        "        if np.random.random() < self.epsilon:\n",
        "            i = np.random.randint(0,len(self.actions))\n",
        "        else: \n",
        "            Q = self.model_network(Variable(state.reshape(1, state.shape[0])))\n",
        "            i = Q.data.argmax()\n",
        "            \n",
        "        self.step_counter += 1 \n",
        "        # decay epsilon after each epoch\n",
        "        if self.epsilon_decay:\n",
        "            if self.step_counter % self.epoch_length == 0:\n",
        "                self.epsilon = max(.01, self.epsilon * .95)\n",
        "        \n",
        "        action = self.actions[i]        \n",
        "        return action     \n",
        "    \n",
        "    def learn(self, state1, action1, reward, state2, done):\n",
        "        self.memory.observe(state1, action1, reward, done)\n",
        "        # start training after 1 epoch\n",
        "        if self.step_counter > self.epoch_length:\n",
        "            self.current_loss = self.update_model()\n",
        "\n",
        "    def init_optimizer(self, model, learning_rate):\n",
        "\n",
        "        optimizer = optimizers.SGD(learning_rate)\n",
        "        # optimizer = optimizers.Adam(alpha=learning_rate)\n",
        "        # optimizer = optimizers.AdaGrad(learning_rate)\n",
        "        # optimizer = optimizers.RMSpropGraves(learning_rate, 0.95, self.momentum, 1e-2)\n",
        "\n",
        "        optimizer.setup(model)\n",
        "        return optimizer\n",
        "    \n",
        "    def update_model(self):\n",
        "        (s, action, reward, s_next, is_terminal) = self.memory.sample_minibatch(self.minibatch_size)\n",
        "\n",
        "        ################# TODO 1: Compute Q targets (max_a' Q_hat(s_next, a')) ###################\n",
        "        Q_hat = self.target_network(s_next)\n",
        "        ### Modify here ###\n",
        "        Q_hat_max = F.reshape(Q_hat[:,0], (self.minibatch_size, 1))\n",
        "        y = Q_hat_max\n",
        "        ###################\n",
        "        ##########################################################################################\n",
        "\n",
        "        ############################## TODO 2: Compute Q(s, action) ##############################\n",
        "        Q_s = self.model_network(s)\n",
        "        ### Modify here ###\n",
        "        Q_s_a = F.reshape(Q_s[:,0], (self.minibatch_size, 1))\n",
        "        ###################\n",
        "        Q_s_a = F.reshape(Q_s_a, (self.minibatch_size, 1))\n",
        "        ##########################################################################################\n",
        "        \n",
        "        ############################## TODO 3: Compute Q-value loss ##############################\n",
        "        error = y - Q_s_a\n",
        "        ### Modify here ### \n",
        "        loss = error\n",
        "        ###################\n",
        "        loss = F.sum(loss) / self.minibatch_size\n",
        "        ##########################################################################################\n",
        "\n",
        "        # perform model update only on the model_network (so that the gradient is not backpropped to the target network!)\n",
        "        self.model_network.zerograds() ## zero out the accumulated gradients in all network parameters\n",
        "        loss.backward()\n",
        "        self.optimizer.update()\n",
        "\n",
        "        # target network tracks the model\n",
        "        for dst, src in zip(self.target_network.params(), self.model_network.params()):\n",
        "            dst.data = self.tau * src.data + (1 - self.tau) * dst.data\n",
        "\n",
        "        return loss.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0m_nnNoTJk3"
      },
      "source": [
        "- Define Q-value function in DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F85cg79UVZT6"
      },
      "outputs": [],
      "source": [
        "class QNetwork(Chain):\n",
        "    \"\"\"The neural network architecture as a Chainer Chain - here: single hidden layer\"\"\"\n",
        "\n",
        "    def __init__(self, obs_size, num_actions, nhidden):\n",
        "        \"\"\"Initialize weights\"\"\"\n",
        "        # use LeCunUniform weight initialization for weights\n",
        "        self.initializer = initializers.LeCunUniform()\n",
        "        self.bias_initializer = initializers.Uniform(1e-4)\n",
        "\n",
        "        super(QNetwork, self).__init__(\n",
        "            feature_layer = L.Linear(obs_size, nhidden,\n",
        "                                initialW = self.initializer,\n",
        "                                initial_bias = self.bias_initializer),\n",
        "            action_values = L.Linear(nhidden, num_actions, \n",
        "                                initialW=self.initializer,\n",
        "                                initial_bias = self.bias_initializer)\n",
        "        )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"implements forward pass\"\"\"\n",
        "        h = F.relu(self.feature_layer(x))\n",
        "        return self.action_values(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpugSNJhTSbm"
      },
      "source": [
        "- Define Replay buffer in DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grcPXjcwVZT7"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory(object):\n",
        "    \"\"\"Implements basic replay memory\"\"\"\n",
        "\n",
        "    def __init__(self, observation_size, max_size):\n",
        "        self.observation_size = observation_size\n",
        "        self.num_observed = 0\n",
        "        self.max_size = max_size\n",
        "        self.samples = {\n",
        "                 'obs'      : np.zeros(self.max_size * 1 * self.observation_size,\n",
        "                                       dtype=np.float32).reshape(self.max_size, 1, self.observation_size),\n",
        "                 'action'   : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
        "                 'reward'   : np.zeros(self.max_size * 1).reshape(self.max_size, 1),\n",
        "                 'terminal' : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
        "               }\n",
        "\n",
        "    def observe(self, state, action, reward, done):\n",
        "        index = self.num_observed % self.max_size\n",
        "        self.samples['obs'][index, :] = state\n",
        "        self.samples['action'][index, :] = action\n",
        "        self.samples['reward'][index, :] = reward\n",
        "        self.samples['terminal'][index, :] = done\n",
        "\n",
        "        self.num_observed += 1\n",
        "\n",
        "    def sample_minibatch(self, minibatch_size):\n",
        "        max_index = min(self.num_observed, self.max_size) - 1\n",
        "        sampled_indices = np.random.randint(max_index, size=minibatch_size)\n",
        "\n",
        "        s      = Variable(np.asarray(self.samples['obs'][sampled_indices, :], dtype=np.float32))\n",
        "        s_next = Variable(np.asarray(self.samples['obs'][sampled_indices+1, :], dtype=np.float32))\n",
        "\n",
        "        a      = Variable(self.samples['action'][sampled_indices].reshape(minibatch_size))\n",
        "        r      = self.samples['reward'][sampled_indices].reshape((minibatch_size, 1))\n",
        "        done   = self.samples['terminal'][sampled_indices].reshape((minibatch_size, 1))\n",
        "\n",
        "        return (s, a, r, s_next, done)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train DQN in 3 different settings\n",
        "- Firstly, let's train DQN with small epsilon (0.01) for 50 iterations."
      ],
      "metadata": {
        "id": "f3APHp1XScw2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt7yEWPKVZT8"
      },
      "outputs": [],
      "source": [
        "interactive = False\n",
        "%matplotlib inline\n",
        "env = SimpleRoomsEnv()\n",
        "agent = DQLearningAgent(range(env.action_space.n), obs_size=16)\n",
        "experiment = Experiment(env, agent)\n",
        "experiment.run_qlearning(50, interactive)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Secondly, let's train DQN with small epsilon (0.01) for 200 iterations."
      ],
      "metadata": {
        "id": "qrntO9O9SsnK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0BrXciIVZT8"
      },
      "outputs": [],
      "source": [
        "interactive = False\n",
        "%matplotlib inline\n",
        "env = SimpleRoomsEnv()\n",
        "agent = DQLearningAgent(range(env.action_space.n), obs_size=16)\n",
        "experiment = Experiment(env, agent)\n",
        "experiment.run_qlearning(200, interactive)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lastly, let's train DQN with decaying epsilon (from 1 to 0.01) for 200 iterations."
      ],
      "metadata": {
        "id": "ZiVAgu1hTCiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYr2A_qnVZT9"
      },
      "outputs": [],
      "source": [
        "interactive = False\n",
        "%matplotlib inline\n",
        "env = SimpleRoomsEnv()\n",
        "agent = DQLearningAgent(range(env.action_space.n), obs_size=16, epsilon=1)\n",
        "experiment = Experiment(env, agent)\n",
        "experiment.run_qlearning(200, interactive)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "1_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}